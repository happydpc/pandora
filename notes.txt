=== Wide Vector BVH traversal ===
AVX2 does not contain all required instructions => emulate (compact is kinda interesting, also because it has 2 options: more compute or using a LUT).



=== VOXELIZATION ===
No library that guarantees watertight results + most Github projects are CLI ==> have to write my own
Going to be based on: http://research.michael-schwarz.com/publ/files/vox-siga10.pdf because its conservative, modern and fast (well parallelizable)

Paper uses xy, xz and yz planes. They do not mention that for the xz plane the edge normal flip with respect to the 3D normal is reversed (flip if y >= 0.0f).
Alternatively they could have used the zx plane in which the flip matches that of xy and yz.
Not mentioned but certainly a good idea: skip projection overlap test for projection planes orthogonal to the triangle plane.

Optimization 1:
Splitting the code into a preprocess and intersect function (like in the paper) is a bad idea. More time is spent on memory allocation than on intersecting (for the Stanford Dragon).

Optimization 2:
Have separate triangle voxelization functions based on the triangles shape (moving code to different functions did not impact performance).
Separate case for triangles whose bounding box covers a single voxel

NOTE: more optimizations can be made which will improve the performance for large (wrt voxels) triangles. For my use-case I assume a low voxel resolution (compared to geometric complexity) so these optimizations are not worth my (development) time right now.

Optimization 3:
SIMD implementation using ISPC. Getting ISPC to compile through CMake is a bit dirty (CMakes fault, man the CMake language is disgusting).
Had to (re)-implement some basic functions because classes are not a thing in ISPC (C).
Wasted some time on the following bug: ISPC float<3> (or any other length) initializer list is different from most other vector languages:
float<3> x = { 1 }      ===>  only initializes first field (instead of broadcasting the value)
Debugging in ISPC is hard since there is no debugger support on Windows (just print statements).


=== Sparse Voxel Octree ===
Paper used for SVO construction:
http://graphics.cs.kuleuven.be/publications/BLD13OCCSVO/BLD13OCCSVO_paper.pdf

Octree node layout based on "Efficient Sparse Voxel Octrees":
http://research.nvidia.com/sites/default/files/pubs/2010-02_Efficient-Sparse-Voxel/laine2010i3d_paper.pdf
The contour part is left out; images in the paper suggest that they are conservative. But it uses more memory, removes the possibility for DAGs, and would not improve occlusion culling by that much.
For now, the jump table is also left out; assuming that 16 bit offsets (from the start of the array) is enough to store the entire tree.

TODO: make leaf nodes 16 bit in size by removing the (unused) first child offset
TODO: improve performance by switching to the more complicated algorithm presented in the paper


=== Traversal ===
Based on:
http://research.nvidia.com/sites/default/files/pubs/2010-02_Efficient-Sparse-Voxel/laine2010i3d_paper.pdf
Stripped out the part about contours.

Algorithm seems to assume rays where tfar is indicated by the length of the direction vector (and tnear = 0.0).
So leave out the tmax = min(tmax, 1.0f) part

Not mentioned in the paper but used explicitely in the code: stack size of 23+1 because scale becomes 24 when it gets be low 1.0

=> Uses normal casting, reinterpret casting, bitshifting and arithmatic to compute the highest differing bit (computing scale in POP). Could use bitscan instead (probably faster + easier to reason about).

Reinterpet int to float to efficiently & precisely calculate scaleExp2


=== Sparse Voxel DAG compression ===
For compression the nodes need to be visited in breadth-first order. The construction method used for the SVO constructs the tree depth first. This means that all nodes of a single level are NOT all adjacent in memory. All nodes of a single level could be extracted through a breadth-first traversal.

Because of performance concerns I plan instead on using a different (slightly more complicated) breadth-first construction algorithm based on the voxelization paper. The paper voxelizes the mesh twice, once for allocating the octree nodes and the second time to fill the leaf nodes. The paper also suggests some changes to make the resulting SVO 6-seperating although the algorithm can be adjusted to make it conservative.

In my work I'm not concerned with parallel construction of the SVO nor with very strict memory requirements because the models are thought to be very compact. So I will keep voxelization completely seperated from SVO construction. In practice, the algorithm skips the first paragraph of section 5.1 and does not keep track of x-neighbours or parent pointers.

NOTE: the data layout suggested in the paper is not sparse (it subdivides to the lowest level) for fully filled regions. I use an extra 8 bit mask (in memory that would have otherwise have been unused) to store which children are leafs (just like in Efficient Sparse Voxel Octrees).


Using relative offsets makes current DAG compression impossible (nodes in the old array are updated to store a reference to a node in the new array of nodes => using relative offsets doesnt work).
Create a separate array of indirection for each SVO (from unique node idx to absolute offset in the new array). This should give us some headroom (number of unique nodes still limited to 16 bits).

NOTE: compressing child nodes & keeping indices to parents misses nodes that are the same but on a different level

This would be so much easier on a single tree; can be integrated into SVO construction

IPSC traversal is a bit wonky. Using pointers instead of values everywhere breaks even the SVO traversal unless 64-bit addressing is enabled. The problem is worse in the DAG version; it seems like the ISPC compiler has problems with reinterpret casting. Solution is to keep it simple: always index from the start of the allocator array. This requires a second stack (1 for the descriptor, 1 for the corresponding descriptor offset).

64 bit leaf nodes are a relatively minor change to the traversal algorithm (although it requires a good understanding of the traversal algorithm). The traversal algorithm still traverses to the same depth but idx now keeps track of the child index two of the parent (in its parent). By sticking those two together we get the morton code of the corresponding bit in the leaf block (64 bit integer). This does require some more branching which degrades performance.




=== IN-CORE BATCHED RAY TRAVERSAL ===
Use "Fast Divergent Ray Traversal by Batching Rays in a BVH" as a base for the pauseable traversal. For ease of implementation start with nodes storing a pointer to each child (children are not contiguous in memory). Change this if it helps us keep the BVH leaf node in 128 bytes (2 cache lines). Also using a single parent pointer for now; might be suboptimal with batching because parent might not be in L1 cache.

Notes on paper:
Ancestor pointers take up a lot of memory. Should try storing exponentially increasing ancestors (1 up, 2 up, 4 up, 8 up, etc...).
Why is the linked list using 32 bytes?
==> Single threaded only

Implementation tested in Crytek Sponza + path tracing because it is aimed at divergence ray performance. Initial results show that the read/write lock contention around the batching creates significant overhead.

See results/in_core_batching_performance.txt for details

Use a thread-local batch while filling and a shared list of (immutable) batches. Also use references in the batch iterator so we can first intersect all rays and then reinsert them into the top-level tree.

Random writing to pixels should use atomics to splat to pixels. Ray intersection/surface interaction computation are split and RayHits store the UV (barycentric) coordinates. This ensures that we never generate a ray hit at the edge of a triangle (as result of a rounding error) for which we cannot generate a surface interaction.

Hard to spot bug with flushing during traversal: it may run new shading tasks so using a thread_local memory arena does not work. So I switched to a MemoryArena for each thread using a shared thread-safe FreeList as backing storage. The first FreeList implementation (using atomic compare_exchange) had race conditions (probably caused by the ABA threading problem) so it now uses a tbb::concurrent_queue as back-end (one extra indirection but that should not be too big of a problem).

Added any-hit traversal for shadow rays. Traversal returns immediately when an intersection is found. Furthermore, child sorting can be disabled when any intersection is accepted. This should save some time in WiVe8 / PauseableBVH4 where SIMD sorting is expensive (permute operation or horizontal min). There are papers that show that a small performance improvement can be achieved by using a sorted (ahead-of-time) traversal order.



=== RUNTIME RENDERER STATISTICS ===
It would be nice to measure some metrics about the running programs to get a better understanding of data flow in the renderer. Literally every library that I could find is focussed on long-running web applications, only a few of which (unofficially) support C++. Handystats is the most light-weight option in that it does not rely on a separate database executable. It however is not updated actively anymore and misses histograms functionality. Prometheus (with prometheus-cpp) provides a more mature platform and it can be combined with Grafana for nice graphs.

The problems with these existing libraries is that they are focussed on long-running web servers/applications. This means that they (except for handystats) store all data on a server that is supposed to run 24/7. For my use case the statistics should be recorded only while the application is running. The stats should be stored in a new file each time the renderer is run. Furthermore, all existing metrics libraries only store snapshot values at fixed time-intervals. Prometheus accepts push-gateways but those only act as a cache between the application and the Prometheus server. This only fixes the issue of statistics being missed when the running time of the application is so short that it falls inbetween two polls. It does not give us the opportunity to record data and user-defined intervals (i.e. between flush operations).

So I plan on developing a SIMPLE statistics/metrics library that forwards its data to Python for further processing / storage. Moving work to Python should keep development time low (as this is not required for my thesis, it's just a "nice to have").


=== IMPORTING ISLAND SCENE ===
I chose to write the importer in Python with the hopes of not having to spend as much development time on it. The importer uses the PLY library to parse / lex the input. PLY mimicks the lex/yacc interface but is implemented in pure Python (using the build-in regex (re) library). The project should be able to import arbitrary PBRT scenes. Reasons for chosing to support the PBRT scene format instead of Disneys proprietary format are better documentation, more available scenes and because the PBRT description of the Island scene has been simplified.

The importer consists of two parts: parsing the PBRT file into memory and outputing the result in a json file that may be read by the renderer. The importer script is also responsible for removing duplicate (ie materials, textures and geometry) and (currently) unsupported PBRT features.

It took about a week to be able to render most pbrt-v3 demo scenes (https://pbrt.org/scenes-v3.html). The Disney Animation Island scene proved to be a challenge for the importer though. Initial attempts showed that parsing speed was so low that it would have taken months to convert the scene. The culprit was parsing large files containing in-line defined triangle meshes. The dataset contains multiple 6GB files that contain a single mesh. Such a mesh defines lists of triangle indices, vertex positions, normals vectors, texture coordinates and more. Parsing these lists requires at least a single regex operation from the end of each token to the end of the file. 

This problem was relatively easily fixed by letting everything inside an array be a single token, which could than be parsed by numpy's fromstring method. The resulting implementation was much faster but now the program ran out of memory before finishing it's work. After some investigation I noticed that numpy's fromstring method, although faster than the large Python regex operations, was not very fast for large arrays. So I ended up writing a simple float array to string parser in C++ using Boost Python (and Boost Python numpy) to interact with Python. This function first loops over the string to count the number of integers/floats it contains. This is used to preallocate the resulting (numpy) array which is then filled by looping over the array a second time and parsing the numbers using the build-in C/C++ functions. For large arrays (>=10MB) the second pass is parallelized resulting in an extra speed-up. In my short testing this results in over a 10x speed-up compared to numpy's fromstring method.

With parsing performance getting to reasonable numbers the second problem was memory consumption. Even when running on a different machine with 32GB of RAM (as opposed to 16GB on my own machine), Python would consume the whole system memory and start paging (which inflicts a huge hit to performance). Counting the memory usage of the global variables indicated that the arrays containing shapes were growing very quickly (although the program would come to a halt before memory would run out because of the computational cost of determining the memory usage). During the parsing phase these lists are exclusively written to. When outputting the (json) Pandora scene these lists are iterated sequentially. This access pattern makes it relatively cheap to move the list to disk, which is what I did. The list is written to in chunks; the chunk is written out to disk (using Pythons build-in pickle module) when it reaches a threshold of 50.000 items.

Another limiting factor was that each individual mesh in the input pbrt files was written to a temporary binary file and then converted again to a regular mesh (obj/ply) file. Although the latter is hard to circumvent, some performance was gained by merging small meshes into a larger file (at least 500MB).

The result of these optimizations is that the parsing phase "only" requires about 8GB of ram (during the parsing of the large 6GB files).


=== OUT OF CORE RENDERING ===
The batching architecture is designed to reduce the amount of times that data has to be loaded from disk. In out-of-core rendering it is extra important that the latency of loading data from disk is hidden as much as possible. Similar to the design of GPUs, threads that are waiting for file I/O (VRAM in the GPU analogy) are swapped with threads that have active work to do. In our case that means making sure that there are always as many threads traversing as there are cores.

This design seems not that easy to implement. Intel TBB was not designed to support asynchronous file I/O. Furthermore, it might make handling the cache harder. If file I/O is asynchronous than all top level leaf nodes might be requested to load before any load is finished. Although the file I/O should not run on the TBB worker threads, it is important to ensure that they do not produce more data than the worker threads can process, otherwise the process will run out of memory or evict data that has not been processed yet (depending on the implementation). Alternatively, one could perform the loading in the TBB worker threads but ask TBB to spawn additional threads. This would completely rely on the operating system to swap in/out threads. This might be slower than reserving one core for running I/O threads. Maybe we should insert two queues inbetween the TBB threads & I/O threads. Limiting the queue size ensures that the TBB threads or I/O threads cannot get ahead of each other.

PLAN:
The Walt Disney Animation Island scene contains a ton of very small geometry (i.e. individual curves). Loading each one individually from disk is very expensive. So when we generate the top-level leaf nodes containing multiple scene objects this will lead to random access disk I/O. What we would like to do is store all information associated with a top-level leaf node in a single file. To accomplish this we can use the EvictableResource factory function to first create an out-of-core representation that loads the original geometry files (usually *.ply or *.obj). The OOCBatchingAccelerationStructure groups these scene objects (based on their bounding box) into top-level leaf nodes. It can than load all associated scene objects from disk, serialize them and write the result to a single file. The scene objects are then replaced by new ones mimicking the originals but with updated OOC geometry(/material?) factories.

NOTE: think about this part because its kinda ugly from a software engineering perspective
NOTE: how does instancing slot into this? top-level leaf nodes ignore them and we serialize them in a second pass (where each instanced scene object gets its own file)?
NOTE: how are area lights stored? They might be needed at any time for effective light sampling (which requires sampling the underlying geometry) => give area lights an owning (shared) ptr to the geometry so that the geometry always stays in-memory?

=> Waiting for multiple (file) operations
Waiting for multiple operations to finish can be achieved with callbacks (that share a single state struct/class, the last operation executes the follow-up work) but is a bit ugly. If functions that do file I/O return their work as a function pointer than we could more easily bundle them into a single callback. Obviously, the cleanest way of combining multiple I/O tasks would be a future based architecture. Using threads for this purpose requires a lot of thread switching (switching to kernel mode and hoping the OS will pick another Pandora thread). My favourite solution for these kind of operations is using coroutines but the only coroutine based file I/O library is LLFIO (AFIOv2 (ASIO for files)) but its still in Alpha and requires the Coroutines TS.

The artist-defined scene objects create a natural grouping of primitives that prevents us from having to create an acceleration structure over all primitives (which wouldnt fit in memory). However they are often too small to be a top-level leaf node. So scene objects are grouped into top-level leaf nodes. The Embree BVH builder is used to create a BVH2 over the scene objects, subtrees are then collapsed untill they contain enough primitives. TODO: don't count instanced primitives if they already occured in the collapsed treelet. 

NOTE: instancing right now is supported by creating duplicates for each top-level leaf node so all data (geometry + BVH) associated with a top-level leaf node can be stored in a single file on disk. Multiple occurences within a top-level leaf node are instanced as usual.



==== MULTIPLE SAMPLES PER PIXEL IN-FLIGHT ====
The sampler structure was set up to follow PBRT which relies heavily on a sequential access pattern. In Pandora all rayHit/miss invocations may be in random order so even the order in which samples finish is non-deterministic. It would make sense to allocate a new sampler for each path (assuming the allocation overhead is negligible). Allocating all samplers ahead of time (num pixels * spp) might use too much memory. Preferibly we control how many samples can be in-flight at a time seperately from the total spp. So the samplers need to be allocated on-the-fly using the pixel location + sample number as seed for the RNG.


==== SCHEDULING ====
The initial scheduling technique performed the acceleration structure traversal in waves. Each wave all top-level leaf nodes with one or more rays was flushed. This meant that objects were loaded, even if they only had a single ray queued. A better scheduling algorithm must improve on this by finding better times to flush nodes, while guaranteering progress.

The original batched traversal paper [Rendering complex scenes with memory-coherent ray tracing] used a cost-benefit value with each voxel (top-level leaf node). The benefit function estimates how much progress will be made towards completion of the computation as a result of processing the voxel. This function was based on the number of rays as well as the weight associated with them (large weights are more likely to spawn new rays (because of russian roulette)). The cost function is based on the amount of geometry, whether it, or a part of it, is in memory.

This scheme requires continuously flushing batches after the initial rays are inserted. The problem with integrating this in our scheme is that we cannot access thread-local batches that are not filled yet. Also, keeping a list of nodes with the highest score is hard in a multithreaded environment (especially when guaranteeing scalability).

IDEA:
Keep the idea of processing in waves so we can safely process non-empty batches. But before processing sort based on cost/profit function (like with Jacco's paper). BUT: don't process all nodes, but only the ones that have at least a certain percentage of rays compared to the node with the most rays. NOTE: have to be carefull that we do not add to many "sync" points (where we wait for all tasks to finish).

UPDATE:
Experimented wtih continuously picking the best node to process such as suggested in "Out-of-core Data Management for Path Tracing on Hybrid Resources". Locking was required to contiuously select the best node from the list of nodes with full batches. Work stopped once there are no more nodes to select, even if there is still work in flight (which might result in new nodes with work). Thread local batches could not be traversed directly. After each iteration, the system would check if there was any work left (either unprocessed full batches or partial batches) and start a new flush again. Partial batches got delayed so much that most of the processing time (on a 1spp image) was spent on flushing partial batches, which would spawn whole paths of rays. Also, this scheduling algorithm requires a lot of locking which is bad for scalability.

Small change: when forwarding partial batches, merge them together into as little batches as possible.

NOTE:
Small change, big impact: only flush nodes that have at least 1/8th the amount of batches as that of the node with the most amount of batches. This prefers flushing nodes with many batches while still being easy to multi thread.


==== Disabling file caching ====
Is it even possible to disable file caching on Windows? Disabling min/max limit does not disable file caching. Setting both to 0 is also not allowed. Adding FILE_FLAG_NO_BUFFERING seems to work, Windows shows 200MB/s with peaks of 600MB/s reads (Samsung 970 EVO 500GB) compared to 0MB/s with buffering enabled. Be warned that disk caching (like SLC cache on MLC/TLC SSD drives) is also at work and cannot be disabled.


==== Instancing & batching ====
So my OOC renders of the Island scene would keep crashing before rendering even started. The issue was that the system would run out of disk space (only a limited amount of ~170Gb available). Instancing had a big say in this: even though all instances in a group would be saved once, duplication between groups resulted in too much data. The system has not been reworked (currently on the "out-of-core-instancing" branch) to store instances seperately in the cache. To ensure that file system indexing doesnt become a bottleneck, instances are batched into files of 500MB (similar to the PBRT importer). The LRU cache class has been adjusted to be able to store different types of objects at once (thank god I can use C++17: variadic templates + std::variant).


==== Improving SVDAG traversal performance ====
SVDAG traversal performance is important to show that the bandwidth savings result in actual savings in render time. So it is very important that SVDAG traversal is implemented efficiently. The current implementation, which is a direct port from the code provided with the Efficient Sparse Voxel Octree paper, was originally designed for GPUs. It contains many bit casts between floats and integers. Although this might be efficient on the GPU, it creates bottlenecks in the CPU pipeline because the integer and floating point units have to wait for eachothers results. 
Baseline performance of Bunny @ 256^3 voxels (RelWithDebInfo): ~57ms per frame

Keeping track of scaleExp2 manually by recomputing seems like a waste of compute time (and requires float/int casting). Instead we could store a simple look-up table that maps an integer scale to a precomputed scaleExp2. Maybe the authors found that this was slower on GPUs, but on the CPU this should almost definetely be faster.
